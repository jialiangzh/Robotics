<!DOCTYPE html>
<html>
  <head>
    <meta charset='utf-8'>
    <meta http-equiv="X-UA-Compatible" content="chrome=1">

    <link rel="stylesheet" type="text/css" href="stylesheets/stylesheet.css" media="screen">
    <link rel="stylesheet" type="text/css" href="stylesheets/pygment_trac.css" media="screen">
    <link rel="stylesheet" type="text/css" href="stylesheets/print.css" media="print">

    <title>Robotic Pet</title>
  </head>

  <body>

    <header>
      <div class="container">
        <h1>Robotic Pet</h1>
        <h2>EE 125 Final Project</h2>

        <section id="downloads">
          <a href="https://github.com/jialiangzh/Robotics/zipball/master" class="btn">Download as .zip</a>
          <a href="https://github.com/jialiangzh/Robotics/tarball/master" class="btn">Download as .tar.gz</a>
          <a href="https://github.com/jialiangzh/Robotics" class="btn btn-github"><span class="icon"></span>View on GitHub</a>
        </section>
      </div>
    </header>

    <div class="container">
      <section id="main_content">
	  
<h3>
<a id="welcome-to-github-pages" class="anchor" href="#welcome-to-github-pages" aria-hidden="true">
	<span class="octicon octicon-link"></span></a>Welcome to Our Robotic Pet Project!</h3>

<h3>	
<a id="Robotic Pet" class="anchor" href="#Robotic Pet" aria-hidden="true">
	<span class="octicon octicon-link"></span></a>Robotic Pet</h3>
	
<div>
<img src="./images/IMG_3686.JPG" alt="RoboticPet" height="400" align="middle">
</div>

<p>
The Robotic Pet is an automated robot that can mimic common pets behaviours, like entertaining with
a ball autonomously and interacting with human gesture freely. As the final project of EE 125 
Introduction to Robotics class, the robotic pet is built on the UC Berkeley TETRIX robot platform.
</p>

<h3>	
<a id="Introduction" class="anchor" href="#Introduction" aria-hidden="true">
	<span class="octicon octicon-link"></span></a>Introduction</h3>

<p>
Our robotic pet project aims to bring happiness and potential convenience to people's everyday life.
Because robots do not have minds or ideas, how to make them "live" like real animals is a quite
interesting topic for both of us.
</p>

<p>
An ideal robotic pet can follow the pet owner like a servant with real time interaction with 
the owner. The robotic pet should also resemble a real pet and bring enjoyment to its owner.
In our project, we specifically investigated on implementing two major functionalities 
for our robotic pet: not only the feature of self-entertaining with a ball, but also owner hand gesture 
comprehension. So in the future, one can possibly use the robot as an extra carrier to lift heavy package,
which would be especially beneficial to elder civilians.
</p>

<!-- <pre><code>$ cd your_repo_root/repo_name
$ git fetch origin
$ git checkout gh-pages
</code></pre> -->

<h3>
<a id="Design" class="anchor" href="#Design" aria-hidden="true">
	<span class="octicon octicon-link"></span></a>Design</h3>
	
<h4>
<a id="Functionality" class="anchor" href="#Functionality" aria-hidden="true">
	<span class="octicon octicon-link"></span></a>Functionality</h4>
	
<ul>
<li>Self-entertaining with a ball</li>
<li>Automatic following with pet's owner</li>
<li>Owner's hand gesture comprehension</li>
</ul>
	
<h4>
<a id="DesignCriteria" class="anchor" href="#DesignCriteria" aria-hidden="true">
	<span class="octicon octicon-link"></span></a>Design Criteria</h4>
	
<ul>
<li>1. Stable self-entertaining mode to mimic a pet</li>
<li>2. Effective hand gesture comprehension and interaction</li>
<li>3. Interactive with pet's owner when possible</li>
<li>4. Reduce communication lagging and response time</li>
<li>5. Durable, robust and safe for users</li>
</ul>

<h4>
<a id="Computer-Vision" class="anchor" href="#Computer-Vision" aria-hidden="true">
	<span class="octicon octicon-link"></span></a>Computer Vision</h4>

<p>
a) Data streaming: Because the board on the robot does not have enough calculating power, we decide to put all heavy computations 
on our laptop. So the first task is to stream the data from the robot camera to our laptop. The package "raspicam_node" really 
helped us on adjusting the framerate and quality of the image so that the laptop can receive the data stream in real time with
little lagging.
</p>

<p>
b) Our second task is to do camera calibration. By computing the homography matrix H, we are able to map the 2-dimensional 
pixel position of the digital camera into the actual floor coordinate (x, y). The robot now can process the object position 
in the ground coordinates accordingly.
</p>

<div>
<img src="./images/Calibration_Graph_smaller.jpg" alt="Calibration_Graph" height="250" align="middle">
</div>

<p>
c) Now we can visualize the ball and the input gesture. This is done by filtering out the environment pixels and using the python cv 
functions to process the target contours and to locate the centre of the target. If other environment noise presents, we should
find the largest bounding box for the contours and to indicate the location. If the largest contour is small enough, then we can say
the target is not present in our image. Lab 6 "Object tracking" in the course gives us the initial idea. We built upon lab 6, 
designed separate mode for the hand gesture and the ball, implemented the bounding box area criteria for checking the presence 
of the object, and designed the appropriate parameters for the Kalman Filter for the linear motion prediction. 
Using ROS publisher, predicted location and the actual location of target object is published and waiting for further processing.
</p>

<h4>
<a id="Control-Signal-Generation" class="anchor" href="#Control-Signal-Generation" aria-hidden="true">
	<span class="octicon octicon-link"></span></a>Control Signal Generation</h4>

<p>
a) For the ball: Our goal is to let the robotic pet entertain the ball by itself. When the image processing publisher tells us that 
there is no object present, we should let the pet rotate to find the new ball position until the ball come into its vision again. 
Then, using feedback control technique, the pet adjusts its facing angle until it aligns itself to the ball. Then by several 
short forward movements, it further adjusts its angle gradually. Finally, it will accelerate with full speed in order to hit the ball. 
The ball is then kicked away and the robot will search it again and repeat the previous process. Our controller code does this
job and publishes velocity control signal remotely to the robot.
</p>

<p>
b) For hand gesture: The pet should be able to get the hand position and keep a certain distance with the hand. When the hand is
far away, it should be able to catch up with the hand; and when the hand is shaking left or right, the pet should rotate according
to the order and direction.
</p>

<h4>
<a id="Materializing-Velocity-Signal" class="anchor" href="#Materializing-Velocity-Signal" aria-hidden="true">
	<span class="octicon octicon-link"></span></a>Materializing Velocity Signal</h4>

<p>
With the I2C tool, we are able to feed the speed information from the Raspberry Pi board to the DC motor. On our Raspberry Pi 
board, we subscribe the velocity command from the ROS Master node, and send binary signal to the predetermined pins on our board. 
Finally the electric signal will be received by the left and the right motor and they will perform as desired.
</p>

<h3>
<a id="Implementation" class="anchor" href="#Implementation" aria-hidden="true">
	<span class="octicon octicon-link"></span></a>Implementation</h3>
	
<h4>
<a id="Hardware" class="anchor" href="#Hardware" aria-hidden="true">
	<span class="octicon octicon-link"></span></a>Hardware</h4>
	
<div>
<img src="./images/IMG_3705.JPG" alt="Hardware1" height="250" align="middle">
</div>

<ul>
<li>Raspberry Pi with camera</li>
<li>Raspberry Pi extension board</li>
</ul>

<div>
<img src="./images/IMG_3702.JPG" alt="Hardware1" height="250" align="middle">
</div>

<ul>
<li>Tetrix Robot frames and wheels</li>
<li>Tetrix DC motor and 12V Battery</li>
</ul>

<h4>
<a id="Software" class="anchor" href="#Software" aria-hidden="true">
	<span class="octicon octicon-link"></span></a>Software</h4>

<ul>
<li>ROS Hydro & Groovy</li>
<li>Debian Wheezy</li>
</ul>

<h4>
<a id="FlowChart" class="anchor" href="#FlowChart" aria-hidden="true">
	<span class="octicon octicon-link"></span></a>Flow Chart</h4>
	
<p>For Self-entertaining with the ball:</p>

<div>
<img src="./images/Entertaining_Ball.jpg" alt="Entertaining_Ball" height="250" align="middle">
</div>

<br>

<p>For owner hand gesture comprehension:</p>

<div>
<img src="./images/Gesture_Comprehension.jpg" alt="Gesture_Comprehension" height="250" align="middle">
</div>

<br>

<h3>
<a id="Results" class="anchor" href="#Results" aria-hidden="true">
	<span class="octicon octicon-link"></span></a>Results</h3>
	
<iframe width="560" height="315" src="//www.youtube.com/embed/AB0WW3HVuuA?rel=0" frameborder="0" allowfullscreen></iframe>

<p>
We achieved success in the robotic pet project after several progresses. Though we were unable to further add in more human gesture
comprehension utilizing more trajectory points and path planning, our basic functionality of a robotic pet worked well as designed.
</p>
	
<h3>
<a id="Conclusion" class="anchor" href="#Conclusion" aria-hidden="true">
	<span class="octicon octicon-link"></span></a>Conclusion</h3>
	
<p>
Our project was quite successful, in consistent with our project goals and designs. However, we did encounter several difficulties
and challenges throughout the project development.
</p>

<p>
First of all, as designed in our project proposal, we planned to use Kinect to realize the function of gesture cognition. Yet after
installing various libraries and dependencies on Raspberry Pi board (like OpenNI etc.), it turned out that this path is totally
blocked due to the incompatibility between Kinect and Debian Wheezy + ROS Groovy. On the other hand, leaving heavy computation on
a single Raspberry Pi board with very limited computing resources is very unwise. After consulting with course GSIs, we changed our
implementation method into distributed system with real time communication utilizing Raspberry Pi camera on-board in order to overcome
the difficulties. In detail, we defined one node as ROS Master (our laptop) and the other node as the TETRIX robot. With the help of
the wireless network router, the TETRIX robot node will only be responsible for image streaming as well as listening and executing the
motor commands. Meanwhile, the master node will be in charge of all the other computations and action planning.
</p>

<p>
This turned out to be very effective, however, a second difficulty came out immediately, which is the huge lagging in back and forth 
communication between the two nodes. This dilemma made the TETRIX robot nearly impossible to track the target and perform real-time,
prompted action according to the fast-changing situations. To solve that issue, we explored various resources online and discovered
a quite functional Raspberry Pi package named <code>raspicam_node</code>. With that, we would be able to experiment and adjust various image
streaming frame rate and individual image quality, yielding a efficient usage of the limited bandwidth and nicely solved the lagging
issue. At the same time, to optimize the performance of publisher and subscriber together with downstream calculations and action
planning, we set each individual publisher and subscriber <code>queuesize = 1</code>. Therefore, there is no longer a huge stack of 
information/commands which are due to limited bandwidth and long executing time existing in he system. Hence, they will not 
overwhelm and confuse the system any more.
</p>

<p>
Future work can be done in adding more features to the robotic pet, refining the human hand gesture comprehension as well as optimizing
the TETRIX motor control and smoothing the robot movements. These will surely be interesting topics to keep working on.
</p>

<h3>
<a id="Team" class="anchor" href="#Team" aria-hidden="true"><span class="octicon octicon-link">
	</span></a>Team</h3>
	
<h4>
<a name="Xingchun-Wang" class="anchor" href="#Xingchun-Wang">
	<span class="octicon octicon-link"></span></a>Xingchun Wang</h4>

<div>
<img src="./images/IMG_3689.JPG" alt="Xingchun" height="220" align="middle">
</div>

<p>
I am a third year Electrical Engineering &amp; Computer Science student at UC Berkeley.
I like to playing piano and video games. This is a very enjoyable class on robotics 
and it's very exciting to see the robot works out after days and nights of working and debugging.
I have learned a lot here on both the theory and the applications. You can contact me 
by shooting me an <a href="acjwxc@126.com">email</a>.</p>

<h4>
<a name="Jialiang-Zhang" class="anchor" href="#Jialiang-Zhang">
	<span class="octicon octicon-link"></span></a>Jialiang Zhang</h4>

<div>
<img src="./images/IMG_3691.JPG" alt="Jialiang" height="200" align="middle">
</div>

<p>
I am a graduate student in the Department of Electrical Engineering &amp; 
Computer Science at UC Berkeley. My interests include Robotics, Embedded Software and
Machine Learning. I did my undergraduate study in Computer Engineering with some
researches on FPGA and embedded systems. This is a very fun class for me to take
and the class project would surely be a valuable experience for my future engineering
career. You are welcome to check out my 
<a href="http://www.linkedin.com/...">LinkedIn</a> page or contact me through 
<a href="jialiang@berkeley.edu">email!</a></p>

<h3>
<a id="Additional-Materials" class="anchor" href="#Additional-Materials" aria-hidden="true">
	<span class="octicon octicon-link"></span></a>Additional Materials</h3>
	
<p>
All the project source codes are available on Github, with download links on top of the page.
Please feel free to check them out!
</p>

<h3>
<a id="References" class="anchor" href="#References" aria-hidden="true">
	<span class="octicon octicon-link"></span></a>References</h3>
	
<p>
• Department of Electrical Engineering and Computer Sciences. 
Aaron Bestick and Austin Buchan. EE C125: Introduction to Robotics Lab Menu. 
University of California, Berkeley. 2014.
</p>

      </section>
    </div>
    
  </body>
</html>
